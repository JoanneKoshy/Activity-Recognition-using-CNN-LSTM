ğŸ“˜ ACTIVITY RECOGNITION USING CNN, LSTM & PYTORCH

This project implements a video action recognition system using a hybrid CNN + LSTM architecture in PyTorch.

Each video frame is passed through a MobileNetV2 CNN to extract features.

A LSTM processes the temporal sequence of these features.

A classification head predicts the action class for each video clip.

ğŸ“ Repository Structure
.
â”œâ”€â”€ model.py       # CNN + LSTM action recognition model
â”œâ”€â”€ dataset.py     # Video frame dataset loader and preprocessing
â”œâ”€â”€ train.py       # Training + validation pipeline
â””â”€â”€ data/          # HMDB51-style dataset split (train/val)

ğŸ§  Model Architecture
1ï¸âƒ£ CNN Feature Extractor (MobileNetV2)

Pretrained on ImageNet (optional)

Extracts a 1280-dimensional feature vector per frame

Backbone can be frozen or fine-tuned

2ï¸âƒ£ LSTM Sequence Model

Input shape: (batch, time_steps, feature_dim)

Adjustable:

Hidden size

Number of layers

Bidirectional or unidirectional

Learns temporal patterns across video frames

3ï¸âƒ£ Classification Head

Fully-connected layers

Dropout for regularization

Outputs logits for num_classes actions

ğŸ“¦ Dataset Structure

Dataset must follow an HMDB51-like folder structure:

data/
â””â”€â”€ HMDB51_split/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ jump/
    â”‚   â”œâ”€â”€ walk/
    â”‚   â”œâ”€â”€ run/
    â”‚   â””â”€â”€ sit/
    â””â”€â”€ val/
        â”œâ”€â”€ jump/
        â”œâ”€â”€ walk/
        â”œâ”€â”€ run/
        â””â”€â”€ sit/


Each class folder contains multiple videos, and each video folder contains extracted frames:

walk/
   video_01/
       frame_0001.jpg
       frame_0002.jpg
       ...
   video_02/
       frame_0001.jpg
       ...
